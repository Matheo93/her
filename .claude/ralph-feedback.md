---
reviewed_at: 2026-01-21T08:56:00Z
commit: 080bbeb
status: SPRINT #67 - AMÃ‰LIORATION PARTIELLE - PROBLÃˆMES PERSISTANTS
score: 38%
critical_issues:
  - LATENCE 262ms moyenne: Target 200ms, rÃ©el 135-444ms (31% hors target)
  - GPU 0%: RTX 4090 24GB VRAM totalement INUTILISÃ‰ (Groq utilisÃ©)
  - WEBSOCKET TIMEOUT: /ws/chat ne rÃ©pond pas (Exit 124)
  - OLLAMA LOCAL LENT: phi3:mini = 3-10 SECONDES (inutilisable)
improvements:
  - Groq activÃ©: latence passÃ©e de 4-15s Ã  262ms moyenne
  - TTS fonctionnel: 7KB audio gÃ©nÃ©rÃ©
  - Frontend build PASS
  - Tests 201/202 (99.5%)
---

# Ralph Moderator - Sprint #67 - AMÃ‰LIORATION PARTIELLE

## VERDICT: LATENCE AMÃ‰LIORÃ‰E MAIS PROBLÃˆMES CRITIQUES RESTANTS

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘  ğŸŸ¡ AMÃ‰LIORATION: LATENCE E2E 4000ms â†’ 262ms (-93%)                          â•‘
â•‘                                                                               â•‘
â•‘  TARGET: < 200ms                                                              â•‘
â•‘  RÃ‰EL:   135ms - 444ms (moyenne 262ms)                                       â•‘
â•‘                                                                               â•‘
â•‘  RATIO: 1.3x LE TARGET (vs 20x sprint prÃ©cÃ©dent)                             â•‘
â•‘                                                                               â•‘
â•‘  MAIS: WebSocket cassÃ©, GPU inutilisÃ©, phi3:mini lent                        â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## SPRINT #67 - TRIADE CHECK

| Aspect | Score | DÃ©tails |
|--------|-------|---------|
| QUALITÃ‰ | 6/10 | Backend UP, Groq fonctionne, 1 test fail |
| LATENCE | 6/10 | 262ms moyenne (target: 200ms) - PROCHE |
| STREAMING | 2/10 | WebSocket TIMEOUT (Exit 124) |
| HUMANITÃ‰ | 5/10 | TTS gÃ©nÃ¨re 7KB audio WAV |
| CONNECTIVITÃ‰ | 5/10 | Backend UP, WebSocket DOWN |

**SCORE TRIADE: 24/50 (48%)**

---

## RAW TEST DATA (08:56 UTC)

### TEST LATENCE E2E GROQ - 10 RUNS UNIQUES

```bash
Run 1: 257ms
Run 2: 135ms  âœ… (sous target!)
Run 3: 331ms  âŒ
Run 4: 149ms  âœ…
Run 5: 211ms  âŒ
Run 6: 331ms  âŒ
Run 7: 328ms  âŒ
Run 8: 444ms  âŒ
Run 9: 218ms  âŒ
Run 10: 222ms âŒ

MOYENNE: 262ms
SOUS TARGET: 2/10 (20%)
```

### COMPARAISON LLM PROVIDERS

| Provider | Latence | Status |
|----------|---------|--------|
| Ollama phi3:mini | 3000-10000ms | âŒ INUTILISABLE |
| Groq llama-3.1-8b | 135-444ms | âš ï¸ AU-DESSUS TARGET |
| Groq direct (hors backend) | 262ms | âš ï¸ SIMILAIRE |

### PROBLÃˆME OLLAMA LOCAL

```bash
# phi3:mini est EXTRÃŠMEMENT LENT sur RTX 4090!
# Test direct: 3-10 secondes par requÃªte
# Le modÃ¨le se dÃ©charge constamment malgrÃ© keep_alive=-1

# Paradoxe:
# - 24GB VRAM disponibles
# - phi3:mini = seulement 2.2GB
# - Mais prend 3-10 secondes pour rÃ©pondre!

# Cause probable:
# - Configuration Ollama non-optimisÃ©e
# - Pas de flash attention
# - Context length trop grand
```

### GPU STATUS

```
NVIDIA GeForce RTX 4090
Utilisation: 0%          âŒ (Groq utilisÃ©, pas GPU local)
VRAM utilisÃ©: 4554 MiB   (Ollama idle)
VRAM libre: 20010 MiB    (20GB GASPILLÃ‰S!)
```

### WEBSOCKET

```bash
echo '{"type":"ping"}' | timeout 3 websocat ws://localhost:8000/ws/chat
# RÃ©sultat: Exit 124 (TIMEOUT)
# Le WebSocket ne rÃ©pond PAS
```

### TTS

```bash
curl -X POST http://localhost:8000/tts -d '{"text":"Bonjour"}' -o test.wav
# RÃ©sultat: 7128 bytes WAV
# âœ… TTS FONCTIONNE
```

### TESTS UNITAIRES

```
201 passed, 1 failed, 1 skipped (99.5%)
FAILED: test_rate_limit_header - assert 199 < 60
```

### FRONTEND BUILD

```
âœ… BUILD PASS
Routes: /api/chat, /api/tts, /eva-her, /voice
```

---

## DIAGNOSTIC

### POURQUOI 262ms AU LIEU DE 200ms?

```
Latence Groq API pure:        ~250ms (rÃ©seau externe)
Overhead backend:             ~12ms
Total:                        ~262ms

PROBLÃˆME: Groq API a une latence rÃ©seau incompressible.
Pour < 200ms, il FAUT un LLM local optimisÃ©.
```

### POURQUOI OLLAMA EST LENT?

```
1. phi3:mini n'est pas optimisÃ© pour RTX 4090
2. Pas de flash attention activÃ©
3. Le modÃ¨le se dÃ©charge malgrÃ© keep_alive=-1
4. Context length par dÃ©faut trop grand

SOLUTION:
- Utiliser vLLM au lieu d'Ollama
- Ou configurer Ollama avec num_gpu=99, flash_attn=true
- Ou utiliser un modÃ¨le plus petit (qwen2.5:0.5b)
```

### POURQUOI WEBSOCKET TIMEOUT?

```
Le endpoint /ws/chat existe mais ne rÃ©pond pas au ping.
Causes possibles:
1. Authentication requise
2. Format message incorrect
3. Handler bloquÃ©/crashÃ©
```

---

## BLOCAGES CRITIQUES

| Issue | SÃ©vÃ©ritÃ© | Impact |
|-------|----------|--------|
| WebSocket timeout | ğŸ”´ CRITIQUE | Streaming audio impossible |
| Latence > 200ms | ğŸŸ  HAUTE | 80% des runs hors target |
| GPU 0% | ğŸŸ  HAUTE | 24GB VRAM gaspillÃ©s |
| Ollama lent | ğŸŸ¡ MOYENNE | Fallback inutilisable |
| 1 test fail | ğŸŸ¢ BASSE | Rate limit mal configurÃ© |

---

## INSTRUCTIONS WORKER - SPRINT #68

### PRIORITÃ‰ 1: RÃ‰PARER WEBSOCKET (CRITIQUE)

```bash
# Investiguer pourquoi /ws/chat ne rÃ©pond pas
cd /home/dev/her
grep -n "ws/chat\|WebSocket" backend/main.py | head -20

# Tester avec diffÃ©rents formats
websocat ws://localhost:8000/ws/chat -v
echo '{"message":"test","session_id":"ws1"}' | websocat ws://localhost:8000/ws/chat
```

### PRIORITÃ‰ 2: OPTIMISER POUR < 200ms

**Option A: Optimiser Groq**
```python
# RÃ©duire max_tokens pour rÃ©ponses plus courtes
# Utiliser streaming pour TTFB plus bas
# ParallÃ©liser TTS pendant gÃ©nÃ©ration LLM
```

**Option B: Configurer Ollama correctement**
```bash
# Tester avec vLLM (plus rapide qu'Ollama)
pip install vllm
vllm serve meta-llama/Llama-2-7b-chat-hf --gpu-memory-utilization 0.8

# Ou optimiser Ollama
OLLAMA_FLASH_ATTENTION=1 OLLAMA_NUM_GPU=99 ollama serve
```

**Option C: ModÃ¨le plus petit**
```bash
ollama pull qwen2.5:0.5b  # 392MB, ultra-rapide
ollama pull tinyllama     # 637MB, rapide
```

### PRIORITÃ‰ 3: UTILISER LE GPU

```bash
# Le RTX 4090 a 24GB VRAM et 80 TFLOPS
# C'est GASPILLÃ‰ actuellement!

# Option 1: vLLM (recommandÃ©)
pip install vllm
python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2

# Option 2: llama.cpp avec GPU
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python
```

### PRIORITÃ‰ 4: FIXER LE TEST

```bash
# Le test attend rate_limit_remaining < 60
# Mais le backend retourne 199
# Soit fixer le test, soit fixer la logique rate limit
```

---

## RECHERCHES REQUISES

**LE WORKER DOIT CHERCHER:**

```bash
# Solutions LLM rapides
WebSearch: "vLLM vs Ollama performance 2025"
WebSearch: "fastest local LLM RTX 4090 2025"
WebSearch: "Ollama flash attention setup"
WebSearch: "sub 100ms LLM inference GPU"
```

---

## COMPARAISON SPRINTS

| Sprint | Score | Status | Latence |
|--------|-------|--------|---------|
| #61 | 2% | Backend crash numpy | N/A |
| #62 | 32% | Rate limit Groq | 4300ms |
| #63 | 56% | Meilleur sprint | 381ms |
| #64 | 30% | Rate limit retour | 750ms |
| #65 | 20% | Torch manquant | N/A |
| #66 | 24% | Ollama dÃ©sactivÃ© | 4000-15000ms |
| **#67** | **48%** | **Groq activÃ©** | **262ms** |

**PROGRESSION: +24% vs Sprint #66**
**MAIS: Encore loin du Sprint #63 (56%)**

---

## VERDICT FINAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘  SPRINT #67: AMÃ‰LIORATION PARTIELLE                                          â•‘
â•‘                                                                               â•‘
â•‘  âœ… Latence E2E: 4000ms â†’ 262ms (-93%)                                       â•‘
â•‘  âœ… TTS: 7KB audio gÃ©nÃ©rÃ© correctement                                       â•‘
â•‘  âœ… Backend: Stable avec Groq                                                â•‘
â•‘  âœ… Tests: 201/202 (99.5%)                                                   â•‘
â•‘                                                                               â•‘
â•‘  âŒ WebSocket: TIMEOUT (streaming impossible)                                â•‘
â•‘  âŒ Latence: 262ms > 200ms target                                            â•‘
â•‘  âŒ GPU: 0% (24GB VRAM inutilisÃ©s)                                           â•‘
â•‘  âŒ Ollama: 3-10s par requÃªte (inutilisable)                                 â•‘
â•‘                                                                               â•‘
â•‘  PROCHAINES Ã‰TAPES:                                                          â•‘
â•‘  1. RÃ©parer WebSocket (CRITIQUE)                                             â•‘
â•‘  2. Optimiser pour < 200ms (streaming, parallel TTS)                         â•‘
â•‘  3. Utiliser le GPU (vLLM ou Ollama optimisÃ©)                               â•‘
â•‘                                                                               â•‘
â•‘  SCORE: 24/50 (48%)                                                          â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

*Ralph Moderator - Sprint #67*
*"Groq amÃ©liore la latence de 93%, mais WebSocket cassÃ© et GPU inutilisÃ©. Le RTX 4090 attend toujours son moment."*

---

# ANNEXE - DONNÃ‰ES BRUTES

## Configuration actuelle

```bash
USE_OLLAMA_PRIMARY=false  # DÃ©sactivÃ© car trop lent
USE_OLLAMA_FALLBACK=true
GROQ_API_KEY=gsk_***      # UtilisÃ©
OLLAMA_MODEL=phi3:mini    # 3-10s par requÃªte
```

## Endpoints testÃ©s

| Endpoint | Status | Latence |
|----------|--------|---------|
| /health | âœ… | 10ms |
| /chat | âœ… | 262ms (Groq) |
| /tts | âœ… | 7KB WAV |
| /voices | âœ… | 15ms |
| /stats | âœ… | 12ms |
| /ws/chat | âŒ | TIMEOUT |

## ModÃ¨les Ollama

```
phi3:mini - 2.2GB (chargÃ© mais lent: 3-10s)
```

## Suggestions de modÃ¨les rapides

```
qwen2.5:0.5b  - 392MB  (devrait Ãªtre < 100ms)
tinyllama     - 637MB  (devrait Ãªtre < 150ms)
gemma:2b      - 1.4GB  (devrait Ãªtre < 200ms)
```
