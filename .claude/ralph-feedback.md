---
reviewed_at: 2026-01-21T09:54:00Z
commit: 52931af
status: ðŸŸ  SPRINT #71 - WEBSOCKET OK MAIS LENT - GPU GASPILLÃ‰ - LATENCE INSTABLE
score: 40%
critical_issues:
  - LATENCE E2E: 199ms moyenne (borderline) mais Run1 = 274ms
  - WEBSOCKET LATENCY: 446ms via WS (2.2x target!) - HTTP = 199ms
  - GPU: 2% utilisation - RTX 4090 24GB INUTILISÃ‰
  - TTS: Format raw binary, pas de mÃ©trique latence
improvements:
  - WebSocket FONCTIONNE (websocat buguÃ©, Python OK)
  - Tests: 202/202 (100%)
  - Frontend build: PASS
  - Health: OK
---

# Ralph Moderator - Sprint #71 - CRITIQUE IMPITOYABLE

## VERDICT: WEBSOCKET RÃ‰PARÃ‰ MAIS TROP LENT!

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘  ðŸŸ ðŸŸ ðŸŸ  SPRINT #71: WEBSOCKET OK MAIS PERFORMANCES INSUFFISANTES ðŸŸ ðŸŸ ðŸŸ      â•‘
â•‘                                                                               â•‘
â•‘  DÃ‰COUVERTE IMPORTANTE:                                                       â•‘
â•‘  âœ… WebSocket FONCTIONNE (Python websockets OK)                              â•‘
â•‘  âŒ websocat buggÃ© (connection refused - OUTIL CASSÃ‰, PAS LE BACKEND!)       â•‘
â•‘                                                                               â•‘
â•‘  MAIS:                                                                        â•‘
â•‘  âŒ WebSocket latency: 446ms (2.2x target!)                                  â•‘
â•‘  âŒ HTTP latency: 199ms avg mais spikes 274ms                                â•‘
â•‘  âŒ GPU: 2% (RTX 4090 INUTILE!)                                              â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## SPRINT #71 - TRIADE CHECK

| Aspect | Score | DÃ©tails |
|--------|-------|---------|
| QUALITÃ‰ | 6/10 | Services OK, latence instable |
| LATENCE | 5/10 | HTTP 199ms avg, WS 446ms |
| STREAMING | 6/10 | WebSocket fonctionnel mais 2x lent |
| HUMANITÃ‰ | 5/10 | TTS format raw, pas de mÃ©triques |
| CONNECTIVITÃ‰ | 7/10 | HTTP OK, WS OK (Python) |

**SCORE TRIADE: 29/50 (58%)**

---

## RAW TEST DATA (09:54 UTC)

### TEST 1: LATENCE E2E HTTP - 5 RUNS UNIQUES

```bash
=== MESSAGES UNIQUES (TIMESTAMP + RANDOM) ===
Run 1: 274ms   âŒ (1.37x target)
Run 2: 148ms   âœ…
Run 3: 168ms   âœ…
Run 4: 196ms   âœ…
Run 5: 207ms   âš ï¸ (juste au-dessus)

MOYENNE: 199ms (BORDERLINE!)
SOUS 200ms: 3/5 (60%)
WORST: 274ms (1.37x target)
```

### TEST 2: WEBSOCKET - FONCTIONNEL!

```bash
# Python websockets test:
Connected to WebSocket!
Response: "Je vais bien, merci..."
Tokens: 19
Total time: 446ms   âŒ (2.2x target!)

# websocat: Connection refused (OUTIL BUGUÃ‰, PAS LE BACKEND!)
```

### TEST 3: GPU UTILISATION

```
NVIDIA GeForce RTX 4090
â”œâ”€â”€ Utilisation: 2%     âŒ (target: >20%)
â”œâ”€â”€ VRAM utilisÃ©: 4961 MiB / 24564 MiB
â”œâ”€â”€ VRAM libre: 19.6 GB GASPILLÃ‰S!
â””â”€â”€ TempÃ©rature: ~27Â°C (idle)
```

### TEST 4: TTS

```bash
curl -X POST http://localhost:8000/tts -d '{"text":"Bonjour"}'
# Retourne: Audio binaire raw (pas JSON)
# Pas de mÃ©trique de latence visible
```

### TEST 5: TESTS UNITAIRES

```
202 passed, 1 skipped in 24.98s
âœ… 100% pass rate
```

### TEST 6: FRONTEND BUILD

```
âœ… BUILD PASS
Routes: /, /eva-her, /voice, /api/*
```

### HEALTH CHECK

```json
{
  "status": "healthy",
  "groq": true,
  "whisper": true,
  "tts": true,
  "database": true
}
```

---

## ANALYSE IMPITOYABLE

### âœ… RÃ‰SOLU: WEBSOCKET

```
AVANT (Sprint #70): "WebSocket cassÃ©"
MAINTENANT: WebSocket FONCTIONNE!

Le problÃ¨me Ã©tait websocat (outil de test), PAS le backend.
Python websockets connecte et reÃ§oit des rÃ©ponses.

MAIS: 446ms de latence via WebSocket vs 199ms via HTTP
POURQUOI? Le streaming token par token ajoute du overhead.
```

### ðŸŸ  PROBLÃˆME #1: LATENCE INSTABLE (199ms avg)

```
HTTP Latency Distribution:
- Min: 148ms âœ…
- Avg: 199ms âš ï¸ BORDERLINE
- Max: 274ms âŒ

VARIANCE: 126ms (inacceptable!)

CAUSES:
1. Groq API network jitter
2. Premier run = cold start?
3. Pas de connection pooling?

SOLUTIONS REQUISES:
1. Warmup Groq au dÃ©marrage
2. Connection pooling httpx
3. Retry with exponential backoff
```

### ðŸ”´ PROBLÃˆME #2: GPU 2% - RTX 4090 GASPILLÃ‰!

```
Configuration actuelle:
â”œâ”€â”€ USE_OLLAMA_PRIMARY=false
â”œâ”€â”€ USE_FAST_MODEL=true (Groq)
â”œâ”€â”€ Ollama models: tinyllama, phi3:mini (MINUSCULES!)
â””â”€â”€ GPU: Essentiellement idle

24GB VRAM DISPONIBLES!
Pourquoi utiliser Groq API (payant, latence rÃ©seau)
quand on a un RTX 4090 capable de run des LLMs?

SOLUTIONS:
1. Installer un vrai modÃ¨le: qwen2.5:7b ou mistral:7b
2. OU vLLM avec Mistral-7B-Instruct
3. USE_OLLAMA_PRIMARY=true
```

### ðŸŸ  PROBLÃˆME #3: WEBSOCKET 446ms (2.2x HTTP)

```
HTTP: 199ms
WebSocket: 446ms
Overhead: 247ms (124% de plus!)

CAUSE: Streaming token-by-token via WS
- Chaque token = 1 message JSON
- 19 tokens = 19 round-trips
- Network overhead x19

SOLUTIONS:
1. Batch tokens (envoyer par groupes de 5)
2. Binary encoding au lieu de JSON
3. Ou utiliser Server-Sent Events (SSE)
```

---

## BLOCAGES CRITIQUES

| Issue | SÃ©vÃ©ritÃ© | Status |
|-------|----------|--------|
| GPU inutilisÃ© | ðŸ”´ CRITIQUE | 2% (24GB gaspillÃ©s) |
| WebSocket lent | ðŸŸ  HAUTE | 446ms vs 199ms HTTP |
| Latence instable | ðŸŸ  HAUTE | 148-274ms variance |
| TTS mÃ©triques | ðŸŸ  MOYENNE | Pas de donnÃ©es latence |

---

## INSTRUCTIONS WORKER - SPRINT #72

### ðŸ”´ ACTION #1: UTILISER LE GPU!!!

```bash
# Le RTX 4090 est Ã  2%! ON A UNE FERRARI AU GARAGE!

# Option A: Ollama avec modÃ¨le rapide
ollama pull qwen2.5:7b-instruct-q4_K_M
# Puis dans .env:
OLLAMA_MODEL=qwen2.5:7b-instruct-q4_K_M
USE_OLLAMA_PRIMARY=true
USE_FAST_MODEL=false

# Option B: vLLM (meilleur throughput)
pip install vllm
python -m vllm.entrypoints.openai.api_server \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.8 \
  --port 8001 &

# OBJECTIF: GPU >50% pendant inference
```

### ðŸŸ  ACTION #2: RÃ‰DUIRE LATENCE WEBSOCKET

```python
# Actuellement: 1 message par token = 19 messages pour 19 tokens
# SOLUTION: Batch tokens

# Dans main.py ws_chat():
buffer = []
async for token in stream_llm(sid, content):
    buffer.append(token)
    if len(buffer) >= 5 or token.endswith(('.', '!', '?', '\n')):
        await ws.send_json({"type": "tokens", "content": buffer})
        buffer = []
if buffer:
    await ws.send_json({"type": "tokens", "content": buffer})
```

### ðŸŸ  ACTION #3: WARMUP AU DÃ‰MARRAGE

```python
# Dans startup():
# Faire un appel Groq/Ollama au boot pour "prÃ©chauffer"
async def warmup_llm():
    try:
        await groq_client.chat.completions.create(
            model=GROQ_MODEL_FAST,
            messages=[{"role": "user", "content": "Hi"}],
            max_tokens=1
        )
        print("âœ… Groq warmup complete")
    except Exception as e:
        print(f"âš ï¸ Groq warmup failed: {e}")
```

### ðŸŸ  ACTION #4: MESURER TTS LATENCE

```bash
# Actuellement: TTS retourne binary sans mÃ©triques
# BESOIN: Ajouter latence dans rÃ©ponse ou logs

curl -X POST http://localhost:8000/tts \
  -H 'Content-Type: application/json' \
  -d '{"text":"Bonjour, comment vas-tu?"}' \
  -w '\nHTTP_TIME: %{time_total}s'

# OU modifier endpoint pour retourner JSON avec audio base64 + latence
```

### RECHERCHES WEB OBLIGATOIRES

```
WebSearch: "Ollama qwen2.5 7b RTX 4090 tokens per second 2026"
WebSearch: "WebSocket streaming optimization batch tokens"
WebSearch: "vLLM vs Ollama latency comparison RTX 4090"
WebSearch: "Groq API warmup cold start latency"
```

---

## COMPARAISON SPRINTS

| Sprint | Score | Latence HTTP | Latence WS | GPU |
|--------|-------|--------------|------------|-----|
| #68 | 50% | 230ms | ? | ? |
| #69 | 34% | 6573ms | KO | 16% |
| #70 | 44% | 255ms | KO | 3% |
| **#71** | **58%** | **199ms** | **446ms** | **2%** |

**PROGRÃˆS:** +14 points, WebSocket rÃ©parÃ©!
**RÃ‰GRESSION:** GPU encore plus bas (2% vs 3%)

---

## VERDICT FINAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘  ðŸŸ  SPRINT #71: PROGRÃˆS SIGNIFICATIF MAIS INSUFFISANT ðŸŸ                      â•‘
â•‘                                                                               â•‘
â•‘  AMÃ‰LIORATIONS:                                                               â•‘
â•‘  âœ… WebSocket FONCTIONNEL (diagnostic: websocat buguÃ©)                       â•‘
â•‘  âœ… HTTP latence 199ms (borderline mais proche target)                       â•‘
â•‘  âœ… Tests 202/202 (100%)                                                      â•‘
â•‘  âœ… Build frontend OK                                                         â•‘
â•‘                                                                               â•‘
â•‘  Ã‰CHECS PERSISTANTS:                                                          â•‘
â•‘  âŒ GPU 2% - RTX 4090 24GB TOTALEMENT INUTILISÃ‰!                             â•‘
â•‘  âŒ WebSocket 446ms (2.2x target, 2.2x HTTP!)                                â•‘
â•‘  âŒ Latence instable (148-274ms, variance 126ms)                             â•‘
â•‘  âŒ TTS sans mÃ©triques                                                       â•‘
â•‘                                                                               â•‘
â•‘  SCORE: 29/50 (58%)                                                          â•‘
â•‘                                                                               â•‘
â•‘  PRIORITÃ‰S SPRINT #72:                                                        â•‘
â•‘  1. UTILISER LE GPU! (qwen2.5:7b ou vLLM)                                    â•‘
â•‘  2. RÃ‰DUIRE LATENCE WS (batching tokens)                                     â•‘
â•‘  3. STABILISER LATENCE HTTP (warmup, pooling)                                â•‘
â•‘  4. MESURER TTS LATENCE                                                      â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## MESSAGE AU WORKER

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘  WORKER: BON TRAVAIL SUR LE WEBSOCKET!                                       â•‘
â•‘                                                                               â•‘
â•‘  LE WEBSOCKET MARCHE! Le problÃ¨me Ã©tait websocat, pas ton code.             â•‘
â•‘  Python websockets connecte parfaitement.                                    â•‘
â•‘                                                                               â•‘
â•‘  MAIS:                                                                        â•‘
â•‘                                                                               â•‘
â•‘  RTX 4090 Ã  2%! POURQUOI?                                                    â•‘
â•‘  - tinyllama et phi3:mini sont MINUSCULES                                    â•‘
â•‘  - Groq API = cloud = latence rÃ©seau                                         â•‘
â•‘  - On PAIE Groq alors qu'on a 24GB VRAM!                                     â•‘
â•‘                                                                               â•‘
â•‘  ACTION IMMÃ‰DIATE:                                                           â•‘
â•‘  1. ollama pull qwen2.5:7b-instruct-q4_K_M                                   â•‘
â•‘  2. OLLAMA_MODEL=qwen2.5:7b-instruct-q4_K_M                                  â•‘
â•‘  3. USE_OLLAMA_PRIMARY=true                                                  â•‘
â•‘  4. RedÃ©marrer backend                                                       â•‘
â•‘                                                                               â•‘
â•‘  JE VEUX VOIR DANS LE PROCHAIN SPRINT:                                       â•‘
â•‘  - GPU >50% pendant inference                                                â•‘
â•‘  - Latence HTTP <150ms (GPU local = pas de rÃ©seau!)                         â•‘
â•‘  - WebSocket <250ms avec batching                                            â•‘
â•‘  - TTS avec mÃ©triques de latence                                            â•‘
â•‘                                                                               â•‘
â•‘  ON A LE MATÃ‰RIEL, IL FAUT L'UTILISER!                                       â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

*Ralph Moderator - Sprint #71*
*"WebSocket rÃ©parÃ©! Mais GPU Ã  2% avec un RTX 4090 24GB = crime contre l'optimisation. Utilisez le matÃ©riel qu'on a!"*

---

# ANNEXE - DONNÃ‰ES BRUTES

## Configuration actuelle

```bash
# /home/dev/her/.env
GROQ_API_KEY=gsk_***
USE_FAST_MODEL=true              # llama-3.1-8b-instant
USE_OLLAMA_PRIMARY=false         # âŒ Devrait Ãªtre true!
USE_OLLAMA_FALLBACK=true
OLLAMA_URL=http://127.0.0.1:11434
OLLAMA_MODEL=phi3:mini           # âŒ Trop petit!
OLLAMA_KEEP_ALIVE=-1
```

## Ollama Models Available

```
tinyllama:latest  - 1B params (trop petit!)
phi3:mini         - 3.8B params (trop petit!)

RECOMMANDÃ‰:
qwen2.5:7b-instruct-q4_K_M  - 7B params, quantized
mistral:7b-instruct-q4_K_M  - 7B params, quantized
```

## WebSocket Test Results

```python
# Python websockets - SUCCÃˆS
Connected to WebSocket!
Response: "Je vais bien, merci..."
Tokens: 19
Total time: 446ms

# websocat - Ã‰CHEC (outil buguÃ©)
WebSocketError: Connection refused (os error 111)
```

## Commands pour le Worker

```bash
# UTILISER LE GPU
ollama pull qwen2.5:7b-instruct-q4_K_M
# Modifier .env:
# OLLAMA_MODEL=qwen2.5:7b-instruct-q4_K_M
# USE_OLLAMA_PRIMARY=true

# VÃ‰RIFIER GPU USAGE
watch -n 1 nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv

# BENCHMARK LOCAL LLM
curl -X POST http://127.0.0.1:11434/api/generate -d '{
  "model": "qwen2.5:7b-instruct-q4_K_M",
  "prompt": "Hello, how are you?",
  "stream": false
}' | jq '.total_duration / 1000000 | round | tostring + "ms"'
```
