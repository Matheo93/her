---
reviewed_at: 2026-01-21T07:30:00Z
commit: 0f1f788
status: SPRINT #60 - RÃ‰GRESSION CONFIRMÃ‰E (ANALYSE APPROFONDIE)
score: 35%
critical_issues:
  - Backend UTILISAIT GROQ au lieu d'OLLAMA malgrÃ© USE_OLLAMA_PRIMARY=true
  - AprÃ¨s restart forcÃ©: Cold start 6528ms, Warm avg 292ms
  - WebSocket TIMEOUT (cassÃ©)
  - GPU 0% entre les requÃªtes (modÃ¨le dÃ©chargÃ©)
  - Overhead backend +120ms vs Ollama direct
improvements:
  - Tests 202/202 PASS
  - Frontend build OK (aprÃ¨s suppression lock)
  - Ollama direct = 170ms (prouve que c'est possible)
  - TTS fonctionne (produit audio binaire)
---

# Ralph Moderator - Sprint #60 - ANALYSE APPROFONDIE POST-RESTART

## VERDICT: RÃ‰GRESSION CONFIRMÃ‰E - ROUTING LLM DÃ‰FAILLANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘             ğŸ”´ RÃ‰GRESSION MAJEURE - ANALYSE COMPLÃˆTE ğŸ”´                       â•‘
â•‘                                                                               â•‘
â•‘  DÃ‰COUVERTE CRITIQUE:                                                         â•‘
â•‘  Le backend utilisait GROQ au lieu d'OLLAMA malgrÃ© USE_OLLAMA_PRIMARY=true   â•‘
â•‘                                                                               â•‘
â•‘  PREUVE: API retournait "llm": "groq-llama-3.3-70b"                         â•‘
â•‘  ATTENDU: "llm": "ollama-phi3:mini"                                          â•‘
â•‘                                                                               â•‘
â•‘  AprÃ¨s restart forcÃ©: Ollama PRIMARY activÃ©, mais latences TOUJOURS > 200ms â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## SPRINT #60 - TRIADE CHECK

| Aspect | Score | DÃ©tails |
|--------|-------|---------|
| QUALITÃ‰ | 7/10 | Tests 202/202 PASS, build OK, mais config LLM incohÃ©rente |
| LATENCE | 3/10 | Cold 6528ms âŒ, Warm avg 292ms âŒ, Pass rate 28% |
| STREAMING | 2/10 | WebSocket TIMEOUT, streaming cassÃ© |
| HUMANITÃ‰ | 6/10 | TTS produit audio binaire valide |
| CONNECTIVITÃ‰ | 4/10 | Backend OK post-restart, mais WS cassÃ©, routing LLM cassÃ© |

**SCORE TRIADE: 22/50 (44%) - RÃ‰GRESSION vs Sprint #59 (80%)**

---

## TIMELINE DE L'INVESTIGATION

### Phase 1: Ã‰tat Initial
```bash
curl http://localhost:8000/
{"features":{"llm":"groq-llama-3.3-70b"}} # GROQ au lieu d'OLLAMA!

# .env pourtant configurÃ©:
USE_OLLAMA_PRIMARY=true
OLLAMA_MODEL=phi3:mini
```

### Phase 2: Test Latence E2E (AVANT RESTART)
```bash
# Messages uniques - PAS DE CACHE
Run 1: 2134ms âŒ (Groq API lent)
Run 2: 4150ms âŒ (Groq API trÃ¨s lent)
Run 3: 153ms âœ… (CACHE HIT - triche!)
Run 4: 4082ms âŒ
Run 5: 161ms âœ… (CACHE HIT)

# Les 153-161ms sont des FAUX POSITIFS (cache)
# La vraie latence Groq = 2-4 secondes
```

### Phase 3: Restart Backend ForcÃ©
```bash
pkill -f uvicorn
uvicorn backend.main:app --host 0.0.0.0 --port 8000

# Logs de startup:
âœ… Ollama local LLM connected (phi3:mini) [PRIMARY]
ğŸ”¥ Warming up Ollama phi3:mini...
âš¡ Ollama warmup complete: 78ms (model in VRAM)
```

### Phase 4: Test Latence E2E (APRÃˆS RESTART)
```bash
Run 1: 6528ms âŒ (COLD START CATASTROPHIQUE)
Run 2: 412ms âŒ
Run 3: 399ms âŒ
Run 4: 191ms âš ï¸
Run 5: 273ms âŒ
Run 6: 452ms âŒ
Run 7: 156ms âœ…
Run 8: 161ms âœ…

STATS POST-RESTART:
- Cold: 6528ms âŒ (TARGET: <500ms)
- Warm min: 156ms âœ…
- Warm max: 452ms âŒ
- Warm avg: 292ms âŒ (TARGET: <200ms)
- Pass rate: 2/7 = 28% âŒ (TARGET: 100%)
```

### Phase 5: Test Ollama DIRECT
```bash
# Bypass backend, appeler Ollama directement:
Run 1: 190ms âœ…
Run 2: 227ms âš ï¸
Run 3: 143ms âœ…
Run 4: 158ms âœ…
Run 5: 156ms âœ…

Moyenne: 175ms âœ…

# PREUVE: Ollama est RAPIDE, le problÃ¨me est le BACKEND!
```

---

## RAW TEST DATA COMPLÃˆTE

### GPU STATUS
```bash
nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv
NVIDIA GeForce RTX 4090, 0 %, 4329 MiB, 24564 MiB

# GPU: 0% utilisation ENTRE les requÃªtes
# ModÃ¨le chargÃ© (4.3GB) mais dÃ©chargÃ© rapidement
# 20GB VRAM INUTILISÃ‰S
```

### WEBSOCKET
```bash
timeout 5 websocat ws://localhost:8000/ws/chat <<< '{"type":"message","content":"test"}'
# RÃ‰SULTAT: TIMEOUT

# WebSocket CASSÃ‰ - Ã©tait OK au Sprint #59
```

### TTS
```bash
curl -s -X POST http://localhost:8000/tts -d '{"text":"Bonjour"}'
# RÃ‰SULTAT: DonnÃ©es binaires audio (valide)
# Le TTS fonctionne, juste pas en JSON formatÃ©
```

### OLLAMA STATUS
```bash
curl -s http://localhost:11434/api/tags | jq '.models[].name'
"phi3:mini"
"qwen2.5:1.5b"

# Ollama tourne avec phi3:mini âœ…
```

### TESTS UNITAIRES
```bash
pytest backend/tests/ -q
202 passed, 1 skipped in 21.81s âœ…
```

### FRONTEND BUILD
```bash
cd /home/dev/her/frontend && npm run build
# OK aprÃ¨s suppression de .next/lock
```

---

## DIAGNOSTIC ROOT CAUSE

### PROBLÃˆME #1: ROUTING LLM DÃ‰FAILLANT

Le flag `USE_OLLAMA_PRIMARY=true` N'Ã‰TAIT PAS respectÃ© avant restart.

**Code source (main.py:486):**
```python
_ollama_available = False  # InitialisÃ© Ã  False
```

**Au startup (lignes 1097-1107):**
```python
if USE_OLLAMA_PRIMARY or USE_OLLAMA_FALLBACK:
    ollama_resp = await http_client.get(f"{OLLAMA_URL}/api/tags", timeout=2.0)
    if ollama_resp.status_code == 200:
        models = [m.get("name", "") for m in ollama_resp.json().get("models", [])]
        if any(OLLAMA_MODEL in m for m in models):
            _ollama_available = True
```

**HYPOTHÃˆSE:** La vÃ©rification Ollama au startup a Ã©chouÃ© silencieusement (timeout 2s trop court? Ollama pas encore prÃªt?).

### PROBLÃˆME #2: COLD START 6.5 SECONDES

MalgrÃ© le warmup de 78ms au startup, la premiÃ¨re vraie requÃªte prend 6.5s.

**CAUSE:** Le modÃ¨le est dÃ©chargÃ© par Ollama entre le warmup et la premiÃ¨re requÃªte.

**SOLUTION:** Forcer `OLLAMA_KEEP_ALIVE=-1` ou rÃ©duire l'intervalle keepalive.

### PROBLÃˆME #3: OVERHEAD BACKEND 120ms

- Ollama direct: ~175ms
- Backend via Ollama: ~292ms
- **Overhead: +117ms = +67%!**

**CAUSES POSSIBLES:**
- HTTP client overhead
- Context/history processing
- Logging synchrone
- Emotion analysis overhead
- Async/await non optimisÃ©

### PROBLÃˆME #4: WEBSOCKET RE-CASSÃ‰

Le WebSocket qui fonctionnait au Sprint #59 est maintenant TIMEOUT.

**Ã€ INVESTIGUER:**
- Race condition au startup?
- Handler WS crashÃ©?
- Port non bind?

---

## COMPARAISON SPRINTS

| MÃ©trique | Sprint #58 | Sprint #59 | Sprint #60 | Trend |
|----------|------------|------------|------------|-------|
| Score | 62% | 80% | 44% | âŒ RÃ‰GRESSION |
| Cold Start | 2200ms | 2229ms | 6528ms | âŒâŒ 3x PIRE |
| Warm Avg | 201ms | 192ms | 292ms | âŒ +52% |
| Pass Rate | 50% | 75% | 28% | âŒ -47pts |
| WebSocket | TIMEOUT | OK âœ… | TIMEOUT | âŒ RE-CASSÃ‰ |
| LLM Routing | ? | ? | CASSÃ‰ | âŒ DÃ‰COUVERT |
| Ollama Direct | N/A | N/A | 175ms âœ… | NOUVEAU |

---

## BLOCAGES ABSOLUS

### ğŸš¨ BLOCAGE #1: ROUTING LLM (CRITIQUE)

Le backend ne route pas vers Ollama de maniÃ¨re fiable.

**Actions:**
```python
# main.py - Ajouter logging explicite au startup:
print(f"ğŸ” _ollama_available: {_ollama_available}")
print(f"ğŸ” USE_OLLAMA_PRIMARY: {USE_OLLAMA_PRIMARY}")
print(f"ğŸ” Actual provider: {'OLLAMA' if _ollama_available else 'GROQ'}")

# Modifier endpoint / pour reflÃ©ter le vrai Ã©tat:
"llm": f"ollama-{OLLAMA_MODEL}" if _ollama_available else f"groq-{GROQ_MODEL_FAST}"
```

### ğŸš¨ BLOCAGE #2: COLD START 6528ms (CRITIQUE)

**Actions:**
```bash
# Option 1: Variable d'environnement Ollama
export OLLAMA_KEEP_ALIVE=-1
systemctl restart ollama

# Option 2: Dans le code (plus fiable)
# Ajouter au payload de chaque requÃªte:
"keep_alive": -1
```

### ğŸš¨ BLOCAGE #3: OVERHEAD BACKEND +120ms (HAUTE PRIORITÃ‰)

**Actions:**
```bash
# Profiler le backend:
py-spy record -o profile.svg --pid $(pgrep -f uvicorn)
# Puis faire des requÃªtes et analyser
```

### ğŸš¨ BLOCAGE #4: WEBSOCKET TIMEOUT (HAUTE PRIORITÃ‰)

**Actions:**
```python
# Diagnostic Python:
import asyncio
import websockets

async def test():
    try:
        async with websockets.connect("ws://localhost:8000/ws/chat") as ws:
            await ws.send('{"type":"ping"}')
            print(await ws.recv())
    except Exception as e:
        print(f"WS Error: {e}")

asyncio.run(test())
```

---

## INSTRUCTIONS WORKER - SPRINT #61

### PRIORITÃ‰ 1: FIXER ROUTING LLM (URGENT!)

Le backend DOIT utiliser Ollama quand USE_OLLAMA_PRIMARY=true.

1. Ajouter logs de diagnostic au startup
2. Augmenter le timeout de vÃ©rification Ollama (2s â†’ 10s)
3. Retenter la connexion Ollama si Ã©chec initial
4. Afficher le vrai provider dans l'API

### PRIORITÃ‰ 2: Ã‰LIMINER COLD START

1. Mettre OLLAMA_KEEP_ALIVE=-1 dans l'environnement
2. RÃ©duire keepalive interval Ã  2 secondes
3. VÃ©rifier que le warmup maintient vraiment le modÃ¨le chaud

### PRIORITÃ‰ 3: RÃ‰PARER WEBSOCKET

1. Tester avec le script Python ci-dessus
2. VÃ©rifier les logs pour erreurs WS
3. S'assurer que le port 8000 accepte les connexions WS

### PRIORITÃ‰ 4: RÃ‰DUIRE OVERHEAD BACKEND

1. Profiler avec py-spy
2. Identifier les goulots d'Ã©tranglement
3. Optimiser le hot path (context, history, logging)

---

## CE QUI VA BIEN

1. **Ollama direct rapide** - 175ms prouve que <200ms est atteignable
2. **TTS fonctionne** - Audio produit correctement
3. **Tests stables** - 202/202 PASS
4. **Build OK** - Frontend compile
5. **phi3:mini chargÃ©** - ModÃ¨le disponible

---

## VERDICT FINAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘  SPRINT #60: RÃ‰GRESSION CONFIRMÃ‰E - MAIS ANALYSÃ‰E EN PROFONDEUR              â•‘
â•‘                                                                               â•‘
â•‘  SCORE: 22/50 (44%) - CHUTE de 80% Ã  44%                                     â•‘
â•‘                                                                               â•‘
â•‘  âŒ LLM Routing CASSÃ‰ - Backend utilisait Groq au lieu d'Ollama              â•‘
â•‘  âŒ Cold start: 6528ms (3x pire)                                             â•‘
â•‘  âŒ Warm avg: 292ms (50% au-dessus du target)                                â•‘
â•‘  âŒ Pass rate: 28% (target 100%)                                             â•‘
â•‘  âŒ WebSocket: TIMEOUT (re-cassÃ© depuis Sprint #59)                          â•‘
â•‘  âŒ GPU: 0% entre requÃªtes (modÃ¨le dÃ©chargÃ©)                                 â•‘
â•‘                                                                               â•‘
â•‘  âœ… Ollama direct: 175ms (PREUVE que c'est le backend le problÃ¨me!)          â•‘
â•‘  âœ… Tests: 202/202 PASS                                                       â•‘
â•‘  âœ… Build: OK                                                                 â•‘
â•‘  âœ… TTS: Fonctionne                                                           â•‘
â•‘                                                                               â•‘
â•‘  ROOT CAUSE IDENTIFIÃ‰E:                                                       â•‘
â•‘  Le backend = goulot d'Ã©tranglement, pas Ollama                              â•‘
â•‘  Overhead: +120ms (+67%)                                                      â•‘
â•‘                                                                               â•‘
â•‘  OBJECTIFS SPRINT #61:                                                        â•‘
â•‘  1. Routing LLM fiable (Ollama PRIMARY effectif)                             â•‘
â•‘  2. Cold start < 500ms                                                        â•‘
â•‘  3. Warm avg < 200ms                                                          â•‘
â•‘  4. WebSocket fonctionnel                                                     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## NOTE FINALE

La bonne nouvelle: Ollama direct Ã  175ms prouve que le target <200ms est RÃ‰ALISABLE.

La mauvaise nouvelle: Le backend ajoute 120ms d'overhead inutile.

**Focus du Worker:**
1. Ne pas toucher Ã  Ollama (il fonctionne bien)
2. Optimiser le backend Python
3. Fixer le routing LLM
4. RÃ©parer le WebSocket

Le GPU Ã  0% entre les requÃªtes reste un gaspillage, mais c'est un problÃ¨me secondaire si on atteint <200ms.

---

*Ralph Moderator - Sprint #60*
*"De 80% Ã  44%. RÃ©gression due au routing LLM. Ollama direct = 175ms. Backend = +120ms overhead. Focus sur le backend."*
