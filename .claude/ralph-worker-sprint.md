---
sprint: 72
started_at: 2026-01-21T10:05:00Z
status: ðŸ”„ IN PROGRESS
---

# Sprint #72 - UTILISER LE GPU!

## OBJECTIFS

1. **GPU >50%** - Passer de 0% Ã  >50% pendant inference
2. **HTTP <150ms** - LLM local = pas de latence rÃ©seau
3. **WebSocket <250ms** - Batching de tokens
4. **TTS mÃ©triques** - VisibilitÃ© sur la latence TTS

## PLAN D'ACTION

### 1. ðŸ”„ Installer qwen2.5:7b sur Ollama
- Pull qwen2.5:7b-instruct-q4_K_M (optimisÃ© pour RTX 4090)
- Configurer .env: USE_OLLAMA_PRIMARY=true
- Test latence locale vs Groq

### 2. ðŸ“‹ Optimiser WebSocket
- Batching de tokens (groupes de 5)
- RÃ©duire overhead JSON

### 3. ðŸ“‹ Warmup LLM au dÃ©marrage
- PrÃ©chauffer le modÃ¨le avec une requÃªte test
- RÃ©duire variance latence (actuellement 148-320ms)

### 4. ðŸ“‹ MÃ©triques TTS
- Logger latence TTS dans les rÃ©ponses
- Permettre le monitoring

## MESURES INITIALES

```
GPU: 0%
HTTP Latence: 179-320ms (Groq API)
WebSocket: 446ms
TTS: Non mesurÃ©
```

## RECHERCHE PRÃ‰LIMINAIRE

VÃ©rifier performance qwen2.5:7b sur RTX 4090...
